# ğŸš€ Quickstart with Python  

**Neural-Forge-Ai** is an advanced AI training framework designed to **simplify model finetuning** across various domains, including **LLM finetuning, text classification, image classification, object detection, and more.** It offers a **user-friendly interface** to train models **locally** or on **Hugging Face Spaces.**  

This guide provides a **quick and easy** way to **train a model using Neural-Forge-Ai in Python.**  

---

## ğŸ“Œ Getting Started  

Neural-Forge-Ai can be installed via pip:  

```bash
pip install neural-forge-ai
The following example demonstrates how to finetune an LLM model using Neural-Forge-Ai in Python:

python
Copy
Edit
import os
from neural_forge_ai.params import LLMTrainingParams
from neural_forge_ai.project import NeuralForgeProject

params = LLMTrainingParams(
    model="meta-llama/Llama-3.2-1B-Instruct",
    data_path="HuggingFaceH4/no_robots",
    chat_template="tokenizer",
    text_column="messages",
    train_split="train",
    trainer="sft",
    epochs=3,
    batch_size=1,
    lr=1e-5,
    peft=True,
    quantization="int4",
    target_modules="all-linear",
    padding="right",
    optimizer="paged_adamw_8bit",
    scheduler="cosine",
    gradient_accumulation=8,
    mixed_precision="bf16",
    merge_adapter=True,
    project_name="neural-forge-llama32-1b-finetune",
    log="tensorboard",
    push_to_hub=True,
    username=os.environ.get("HF_USERNAME"),
    token=os.environ.get("HF_TOKEN"),
)

backend = "local"
project = NeuralForgeProject(params=params, backend=backend, process=True)
project.create()
ğŸ”¹ Explanation:
We finetune the meta-llama/Llama-3.2-1B-Instruct model on the HuggingFaceH4/no_robots dataset.
The model is trained for 3 epochs with a batch size of 1 and learning rate of 1e-5.
Optimizer: paged_adamw_8bit | Scheduler: cosine
Gradient Accumulation: 8 | Mixed Precision: bf16
The final trained model will be pushed to Hugging Face Hub after training.
ğŸ”¥ Running the Training
To start the training, run:

bash
Copy
Edit
export HF_USERNAME=<your-hf-username>
export HF_TOKEN=<your-hf-write-token>
python train.py
This will: âœ” Create a new project directory neural-forge-llama32-1b-finetune
âœ” Initiate training using Neural-Forge-Ai
âœ” Push the final trained model to Hugging Face Hub

ğŸ“ NOTE:
Your HF_TOKEN and HF_USERNAME are required only if you:

Want to push the trained model to the Hugging Face Hub.
Are using a gated model or dataset.
ğŸ“œ NeuralForgeProject Class
[[nuraldoc]] project.NeuralForgeProject

âš™ï¸ Parameters
ğŸ“ Text-Based Tasks
[[nuraldoc]] trainers.clm.params.LLMTrainingParams
[[nuraldoc]] trainers.sent_transformers.params.SentenceTransformersParams
[[nuraldoc]] trainers.seq2seq.params.Seq2SeqParams
[[nuraldoc]] trainers.token_classification.params.TokenClassificationParams
[[nuraldoc]] trainers.extractive_question_answering.params.ExtractiveQuestionAnsweringParams
[[nuraldoc]] trainers.text_classification.params.TextClassificationParams
[[nuraldoc]] trainers.text_regression.params.TextRegressionParams

ğŸ–¼ï¸ Image-Based Tasks
[[nuraldoc]] trainers.image_classification.params.ImageClassificationParams
[[nuraldoc]] trainers.image_regression.params.ImageRegressionParams
[[nuraldoc]] trainers.object_detection.params.ObjectDetectionParams

ğŸ“Š Tabular-Based Tasks
[[nuraldoc]] trainers.tabular.params.TabularParams

âœ… Why Neural-Forge-Ai?
No-Code & Low-Code AI Training â€“ Use UI-based workflow or Python API.
Multi-Domain Support â€“ LLMs, Text, Images, Object Detection, and more!
Optimized for GPUs â€“ Faster finetuning & model deployment.
Cloud & Local Training â€“ Flexibility to train anywhere.
Seamless Hugging Face Integration â€“ Push models with a single command.
ğŸš€ Get started with Neural-Forge-Ai and unlock the power of AI today!
