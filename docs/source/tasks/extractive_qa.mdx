# 📖 Extractive Question Answering with Neural-Forge-Ai  

Extractive Question Answering (QA) enables AI models to **find and extract precise answers** from text passages.  
This guide walks you through training **custom QA models** using **Neural-Forge-Ai**, supporting popular architectures  
like **BERT, RoBERTa, and DeBERTa**.  

---

## 🤔 What is Extractive Question Answering?  

Extractive QA models are designed to:  
✅ **Locate exact answer spans** within longer text passages.  
✅ **Understand questions** and match them to relevant context.  
✅ **Extract precise answers** rather than generating them.  
✅ **Handle both simple and complex queries** about the text.  

---

## 📂 Preparing Your Data  

Your dataset must have these essential columns:  

- **`context`**: The passage containing potential answers (also called context).  
- **`question`**: The query you want to answer.  
- **`answers`**: Answer span information including text and position.  

### 📝 Example Dataset Format  

```json
{"context":"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.","question":"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?","answers":{"text":["Saint Bernadette Soubirous"],"answer_start":[515]}}
✅ Note:
The preferred format for question answering is JSONL.
If using CSV, the answer column should be stringified JSON with keys:

"text" (answer string)
"answer_start" (start position in the text)
🔹 Example Dataset from Hugging Face Hub:
📌 lhoestq/squad

🔹 Supported Formats:
You can use both SQuAD and SQuAD v2 data formats with correct column mappings.

🚀 Training Options
🖥️ Local Training (Run on Your Own Hardware)
To train an Extractive QA model locally, create a config file:

 
task: extractive-qa
base_model: google-bert/bert-base-uncased
project_name: neural-forge-bert-ex-qa
log: tensorboard
backend: local

data:
  path: lhoestq/squad
  train_split: train
  valid_split: validation
  column_mapping:
    text_column: context
    question_column: question
    answer_column: answers

params:
  max_seq_length: 512
  max_doc_stride: 128
  epochs: 3
  batch_size: 4
  lr: 2e-5
  optimizer: adamw_torch
  scheduler: linear
  gradient_accumulation: 1
  mixed_precision: fp16

hub:
  username: ${HF_USERNAME}
  token: ${HF_TOKEN}
  push_to_hub: true
▶️ Running the Training Locally
bash
Copy
Edit
$ neural-forge-ai --config config.yaml
🔹 Explanation
Model: google-bert/bert-base-uncased
Dataset: lhoestq/squad
Training Duration: 3 epochs
Batch Size: 4
Learning Rate: 2e-5
Optimizer: adamw_torch
Scheduler: linear
Logging: TensorBoard
Backend: Local Training
Model Upload: Pushed to Hugging Face Hub after training
☁️ Cloud Training on Hugging Face
Want to train models on Hugging Face's Cloud Infrastructure?
Neural-Forge-Ai supports scalable training with GPU acceleration.



✅ Ensure proper column mapping before training!

🔧 Parameter Reference
[[nuraldoc]] trainers.extractive_question_answering.params.ExtractiveQuestionAnsweringParams

🎯 Why Use Neural-Forge-Ai?
✅ No-Code & Low-Code AI Training – UI-based workflow or Python API.
✅ Supports Multiple Domains – LLMs, Text, Images, Object Detection, etc.
✅ Optimized for GPUs – Fast finetuning & model deployment.
✅ Flexible Training – Train locally or on Hugging Face Spaces.
✅ Seamless Hugging Face Integration – Push models with one command.

🚀 Get started with Neural-Forge-Ai and build powerful QA models today!